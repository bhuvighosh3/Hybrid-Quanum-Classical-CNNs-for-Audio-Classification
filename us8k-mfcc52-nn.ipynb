{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.read_csv(\"urbansound8k_mfcc/mfcc52_x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = pd.read_csv(\"urbansound8k_mfcc/mfcc52_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>Feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_43</th>\n",
       "      <th>Feature_44</th>\n",
       "      <th>Feature_45</th>\n",
       "      <th>Feature_46</th>\n",
       "      <th>Feature_47</th>\n",
       "      <th>Feature_48</th>\n",
       "      <th>Feature_49</th>\n",
       "      <th>Feature_50</th>\n",
       "      <th>Feature_51</th>\n",
       "      <th>Feature_52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-212.0</td>\n",
       "      <td>62.6</td>\n",
       "      <td>-123.0</td>\n",
       "      <td>-60.7</td>\n",
       "      <td>-13.900</td>\n",
       "      <td>-29.800</td>\n",
       "      <td>-3.98</td>\n",
       "      <td>11.700</td>\n",
       "      <td>13.000</td>\n",
       "      <td>8.34</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.76</td>\n",
       "      <td>-4.4000</td>\n",
       "      <td>-5.720</td>\n",
       "      <td>0.375</td>\n",
       "      <td>-5.80</td>\n",
       "      <td>1.2300</td>\n",
       "      <td>-5.300</td>\n",
       "      <td>2.220</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>-2.490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-417.0</td>\n",
       "      <td>99.3</td>\n",
       "      <td>-43.0</td>\n",
       "      <td>51.1</td>\n",
       "      <td>9.850</td>\n",
       "      <td>7.970</td>\n",
       "      <td>11.20</td>\n",
       "      <td>1.930</td>\n",
       "      <td>7.030</td>\n",
       "      <td>4.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.62</td>\n",
       "      <td>1.2000</td>\n",
       "      <td>-1.970</td>\n",
       "      <td>1.420</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>1.1600</td>\n",
       "      <td>-1.960</td>\n",
       "      <td>0.416</td>\n",
       "      <td>-1.380</td>\n",
       "      <td>2.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-452.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>-37.6</td>\n",
       "      <td>43.2</td>\n",
       "      <td>8.630</td>\n",
       "      <td>15.400</td>\n",
       "      <td>16.90</td>\n",
       "      <td>1.230</td>\n",
       "      <td>6.830</td>\n",
       "      <td>3.90</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>-0.2630</td>\n",
       "      <td>-1.220</td>\n",
       "      <td>0.395</td>\n",
       "      <td>-3.04</td>\n",
       "      <td>0.6010</td>\n",
       "      <td>-2.660</td>\n",
       "      <td>0.746</td>\n",
       "      <td>-1.870</td>\n",
       "      <td>1.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-406.0</td>\n",
       "      <td>91.2</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>42.8</td>\n",
       "      <td>11.600</td>\n",
       "      <td>5.050</td>\n",
       "      <td>12.40</td>\n",
       "      <td>-1.600</td>\n",
       "      <td>6.660</td>\n",
       "      <td>1.44</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>-0.0941</td>\n",
       "      <td>-0.778</td>\n",
       "      <td>1.150</td>\n",
       "      <td>-2.66</td>\n",
       "      <td>0.7280</td>\n",
       "      <td>-2.420</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-440.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>-42.7</td>\n",
       "      <td>50.7</td>\n",
       "      <td>12.200</td>\n",
       "      <td>15.900</td>\n",
       "      <td>11.70</td>\n",
       "      <td>1.530</td>\n",
       "      <td>11.300</td>\n",
       "      <td>2.55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.6140</td>\n",
       "      <td>-1.230</td>\n",
       "      <td>1.230</td>\n",
       "      <td>-1.53</td>\n",
       "      <td>0.5590</td>\n",
       "      <td>-0.303</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.729</td>\n",
       "      <td>1.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8727</th>\n",
       "      <td>-391.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>-40.6</td>\n",
       "      <td>25.9</td>\n",
       "      <td>1.730</td>\n",
       "      <td>5.580</td>\n",
       "      <td>16.90</td>\n",
       "      <td>7.890</td>\n",
       "      <td>3.490</td>\n",
       "      <td>-4.57</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.50</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>-1.960</td>\n",
       "      <td>1.530</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>-0.0476</td>\n",
       "      <td>-2.500</td>\n",
       "      <td>3.760</td>\n",
       "      <td>-2.420</td>\n",
       "      <td>0.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8728</th>\n",
       "      <td>-339.0</td>\n",
       "      <td>76.4</td>\n",
       "      <td>-35.3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-23.000</td>\n",
       "      <td>-13.600</td>\n",
       "      <td>27.90</td>\n",
       "      <td>-9.220</td>\n",
       "      <td>-5.670</td>\n",
       "      <td>13.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.84</td>\n",
       "      <td>-5.3000</td>\n",
       "      <td>1.090</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-7.31</td>\n",
       "      <td>4.2200</td>\n",
       "      <td>1.050</td>\n",
       "      <td>-5.420</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>7.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729</th>\n",
       "      <td>-297.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-36.8</td>\n",
       "      <td>26.8</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>3.960</td>\n",
       "      <td>11.10</td>\n",
       "      <td>1.910</td>\n",
       "      <td>0.677</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.46</td>\n",
       "      <td>-0.4380</td>\n",
       "      <td>-4.350</td>\n",
       "      <td>-1.020</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>2.2000</td>\n",
       "      <td>1.950</td>\n",
       "      <td>-0.341</td>\n",
       "      <td>-7.150</td>\n",
       "      <td>-3.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8730</th>\n",
       "      <td>-337.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>-44.8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-9.890</td>\n",
       "      <td>-2.150</td>\n",
       "      <td>22.70</td>\n",
       "      <td>0.234</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>10.20</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.26</td>\n",
       "      <td>-2.8400</td>\n",
       "      <td>-4.810</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-3.81</td>\n",
       "      <td>0.3530</td>\n",
       "      <td>-1.260</td>\n",
       "      <td>-1.700</td>\n",
       "      <td>-2.590</td>\n",
       "      <td>1.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8731</th>\n",
       "      <td>-307.0</td>\n",
       "      <td>83.3</td>\n",
       "      <td>-25.8</td>\n",
       "      <td>35.4</td>\n",
       "      <td>4.410</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>9.02</td>\n",
       "      <td>8.210</td>\n",
       "      <td>-1.970</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.00</td>\n",
       "      <td>-3.3100</td>\n",
       "      <td>0.232</td>\n",
       "      <td>-3.280</td>\n",
       "      <td>-7.63</td>\n",
       "      <td>4.9000</td>\n",
       "      <td>-4.300</td>\n",
       "      <td>-7.420</td>\n",
       "      <td>1.590</td>\n",
       "      <td>11.200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8732 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0        -212.0       62.6     -123.0      -60.7    -13.900    -29.800   \n",
       "1        -417.0       99.3      -43.0       51.1      9.850      7.970   \n",
       "2        -452.0      112.0      -37.6       43.2      8.630     15.400   \n",
       "3        -406.0       91.2      -25.0       42.8     11.600      5.050   \n",
       "4        -440.0      104.0      -42.7       50.7     12.200     15.900   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "8727     -391.0      125.0      -40.6       25.9      1.730      5.580   \n",
       "8728     -339.0       76.4      -35.3       42.0    -23.000    -13.600   \n",
       "8729     -297.0      102.0      -36.8       26.8     -0.249      3.960   \n",
       "8730     -337.0      115.0      -44.8       25.0     -9.890     -2.150   \n",
       "8731     -307.0       83.3      -25.8       35.4      4.410     -0.893   \n",
       "\n",
       "      Feature_7  Feature_8  Feature_9  Feature_10  ...  Feature_43  \\\n",
       "0         -3.98     11.700     13.000        8.34  ...       -4.76   \n",
       "1         11.20      1.930      7.030        4.27  ...       -2.62   \n",
       "2         16.90      1.230      6.830        3.90  ...       -2.54   \n",
       "3         12.40     -1.600      6.660        1.44  ...       -1.65   \n",
       "4         11.70      1.530     11.300        2.55  ...        0.76   \n",
       "...         ...        ...        ...         ...  ...         ...   \n",
       "8727      16.90      7.890      3.490       -4.57  ...       -3.50   \n",
       "8728      27.90     -9.220     -5.670       13.00  ...       -5.84   \n",
       "8729      11.10      1.910      0.677       -1.80  ...       -2.46   \n",
       "8730      22.70      0.234     -0.123       10.20  ...       -3.26   \n",
       "8731       9.02      8.210     -1.970       -1.94  ...      -11.00   \n",
       "\n",
       "      Feature_44  Feature_45  Feature_46  Feature_47  Feature_48  Feature_49  \\\n",
       "0        -4.4000      -5.720       0.375       -5.80      1.2300      -5.300   \n",
       "1         1.2000      -1.970       1.420       -1.29      1.1600      -1.960   \n",
       "2        -0.2630      -1.220       0.395       -3.04      0.6010      -2.660   \n",
       "3        -0.0941      -0.778       1.150       -2.66      0.7280      -2.420   \n",
       "4         0.6140      -1.230       1.230       -1.53      0.5590      -0.303   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "8727      0.2410      -1.960       1.530       -1.52     -0.0476      -2.500   \n",
       "8728     -5.3000       1.090       0.322       -7.31      4.2200       1.050   \n",
       "8729     -0.4380      -4.350      -1.020       -1.70      2.2000       1.950   \n",
       "8730     -2.8400      -4.810      -0.192       -3.81      0.3530      -1.260   \n",
       "8731     -3.3100       0.232      -3.280       -7.63      4.9000      -4.300   \n",
       "\n",
       "      Feature_50  Feature_51  Feature_52  \n",
       "0          2.220      -1.370      -2.490  \n",
       "1          0.416      -1.380       2.070  \n",
       "2          0.746      -1.870       1.860  \n",
       "3          0.897       0.503       0.693  \n",
       "4          1.000      -0.729       1.910  \n",
       "...          ...         ...         ...  \n",
       "8727       3.760      -2.420       0.925  \n",
       "8728      -5.420      -1.120       7.280  \n",
       "8729      -0.341      -7.150      -3.020  \n",
       "8730      -1.700      -2.590       1.920  \n",
       "8731      -7.420       1.590      11.200  \n",
       "\n",
       "[8732 rows x 52 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8727</th>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8728</th>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729</th>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8730</th>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8731</th>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8732 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1\n",
       "0             dog_bark\n",
       "1     children_playing\n",
       "2     children_playing\n",
       "3     children_playing\n",
       "4     children_playing\n",
       "...                ...\n",
       "8727          car_horn\n",
       "8728          car_horn\n",
       "8729          car_horn\n",
       "8730          car_horn\n",
       "8731          car_horn\n",
       "\n",
       "[8732 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df_x).reshape(-1, 52, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yash Thakar\\PROGRAMMING\\Quantum\\env\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    1    2    3    4    5    6    7    8    9]\n",
      " [1000  429 1000 1000 1000 1000  374 1000  929 1000]]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(y_encoded, return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, \n",
    "                                                    test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yash Thakar\\PROGRAMMING\\Quantum\\env\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.1551 - loss: 2.7619 - val_accuracy: 0.3921 - val_loss: 1.9160\n",
      "Epoch 2/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3239 - loss: 1.9160 - val_accuracy: 0.5243 - val_loss: 1.5185\n",
      "Epoch 3/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4295 - loss: 1.6577 - val_accuracy: 0.6096 - val_loss: 1.2781\n",
      "Epoch 4/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5038 - loss: 1.4711 - val_accuracy: 0.6812 - val_loss: 1.0912\n",
      "Epoch 5/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5449 - loss: 1.3350 - val_accuracy: 0.7012 - val_loss: 1.0085\n",
      "Epoch 6/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5780 - loss: 1.2501 - val_accuracy: 0.7281 - val_loss: 0.9258\n",
      "Epoch 7/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6084 - loss: 1.1507 - val_accuracy: 0.7619 - val_loss: 0.8186\n",
      "Epoch 8/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6438 - loss: 1.0685 - val_accuracy: 0.7710 - val_loss: 0.7688\n",
      "Epoch 9/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6635 - loss: 1.0230 - val_accuracy: 0.7894 - val_loss: 0.7275\n",
      "Epoch 10/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6844 - loss: 0.9622 - val_accuracy: 0.7991 - val_loss: 0.7120\n",
      "Epoch 11/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6864 - loss: 0.9459 - val_accuracy: 0.8048 - val_loss: 0.6700\n",
      "Epoch 12/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7077 - loss: 0.9071 - val_accuracy: 0.8019 - val_loss: 0.6277\n",
      "Epoch 13/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7139 - loss: 0.8581 - val_accuracy: 0.8134 - val_loss: 0.6076\n",
      "Epoch 14/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7388 - loss: 0.8041 - val_accuracy: 0.8105 - val_loss: 0.6026\n",
      "Epoch 15/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7345 - loss: 0.8016 - val_accuracy: 0.8248 - val_loss: 0.5555\n",
      "Epoch 16/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7563 - loss: 0.7308 - val_accuracy: 0.8403 - val_loss: 0.5468\n",
      "Epoch 17/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7624 - loss: 0.7290 - val_accuracy: 0.8357 - val_loss: 0.5453\n",
      "Epoch 18/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7766 - loss: 0.6792 - val_accuracy: 0.8454 - val_loss: 0.5190\n",
      "Epoch 19/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7855 - loss: 0.6390 - val_accuracy: 0.8392 - val_loss: 0.5262\n",
      "Epoch 20/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7944 - loss: 0.6427 - val_accuracy: 0.8420 - val_loss: 0.5025\n",
      "Epoch 21/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7951 - loss: 0.6152 - val_accuracy: 0.8552 - val_loss: 0.4753\n",
      "Epoch 22/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8145 - loss: 0.5759 - val_accuracy: 0.8535 - val_loss: 0.4772\n",
      "Epoch 23/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8153 - loss: 0.5713 - val_accuracy: 0.8477 - val_loss: 0.4675\n",
      "Epoch 24/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8163 - loss: 0.5560 - val_accuracy: 0.8655 - val_loss: 0.4632\n",
      "Epoch 25/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8156 - loss: 0.5471 - val_accuracy: 0.8638 - val_loss: 0.4573\n",
      "Epoch 26/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8223 - loss: 0.5327 - val_accuracy: 0.8701 - val_loss: 0.4581\n",
      "Epoch 27/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8270 - loss: 0.5289 - val_accuracy: 0.8609 - val_loss: 0.4550\n",
      "Epoch 28/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8316 - loss: 0.5019 - val_accuracy: 0.8683 - val_loss: 0.4590\n",
      "Epoch 29/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8364 - loss: 0.4962 - val_accuracy: 0.8712 - val_loss: 0.4509\n",
      "Epoch 30/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8535 - loss: 0.4529 - val_accuracy: 0.8672 - val_loss: 0.4676\n",
      "Epoch 31/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8378 - loss: 0.4921 - val_accuracy: 0.8712 - val_loss: 0.4653\n",
      "Epoch 32/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8563 - loss: 0.4368 - val_accuracy: 0.8781 - val_loss: 0.4239\n",
      "Epoch 33/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8478 - loss: 0.4482 - val_accuracy: 0.8735 - val_loss: 0.4653\n",
      "Epoch 34/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8653 - loss: 0.4213 - val_accuracy: 0.8678 - val_loss: 0.4549\n",
      "Epoch 35/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8501 - loss: 0.4548 - val_accuracy: 0.8689 - val_loss: 0.4455\n",
      "Epoch 36/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8687 - loss: 0.3997 - val_accuracy: 0.8809 - val_loss: 0.4434\n",
      "Epoch 37/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8749 - loss: 0.3918 - val_accuracy: 0.8758 - val_loss: 0.4304\n",
      "Epoch 38/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8668 - loss: 0.4050 - val_accuracy: 0.8712 - val_loss: 0.4542\n",
      "Epoch 39/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8786 - loss: 0.3764 - val_accuracy: 0.8769 - val_loss: 0.4319\n",
      "Epoch 40/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8745 - loss: 0.3741 - val_accuracy: 0.8878 - val_loss: 0.4175\n",
      "Epoch 41/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8775 - loss: 0.3676 - val_accuracy: 0.8895 - val_loss: 0.4110\n",
      "Epoch 42/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8775 - loss: 0.3744 - val_accuracy: 0.8855 - val_loss: 0.4375\n",
      "Epoch 43/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8833 - loss: 0.3529 - val_accuracy: 0.8861 - val_loss: 0.4274\n",
      "Epoch 44/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8850 - loss: 0.3511 - val_accuracy: 0.8884 - val_loss: 0.4219\n",
      "Epoch 45/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8765 - loss: 0.3524 - val_accuracy: 0.8912 - val_loss: 0.4151\n",
      "Epoch 46/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8890 - loss: 0.3256 - val_accuracy: 0.8855 - val_loss: 0.4246\n",
      "Epoch 47/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8843 - loss: 0.3338 - val_accuracy: 0.8878 - val_loss: 0.4218\n",
      "Epoch 48/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8890 - loss: 0.3199 - val_accuracy: 0.8855 - val_loss: 0.4436\n",
      "Epoch 49/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8962 - loss: 0.3359 - val_accuracy: 0.8827 - val_loss: 0.4626\n",
      "Epoch 50/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8875 - loss: 0.3136 - val_accuracy: 0.8947 - val_loss: 0.4192\n",
      "Epoch 51/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8891 - loss: 0.3177 - val_accuracy: 0.8884 - val_loss: 0.4363\n",
      "Epoch 52/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8965 - loss: 0.3099 - val_accuracy: 0.8878 - val_loss: 0.4535\n",
      "Epoch 53/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8951 - loss: 0.3078 - val_accuracy: 0.8832 - val_loss: 0.4432\n",
      "Epoch 54/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9049 - loss: 0.2849 - val_accuracy: 0.8895 - val_loss: 0.4209\n",
      "Epoch 55/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9023 - loss: 0.3100 - val_accuracy: 0.8970 - val_loss: 0.4291\n",
      "Epoch 56/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9003 - loss: 0.2831 - val_accuracy: 0.8872 - val_loss: 0.4519\n",
      "Epoch 57/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9115 - loss: 0.2708 - val_accuracy: 0.8958 - val_loss: 0.4466\n",
      "Epoch 58/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9105 - loss: 0.2720 - val_accuracy: 0.8918 - val_loss: 0.4439\n",
      "Epoch 59/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9188 - loss: 0.2583 - val_accuracy: 0.8935 - val_loss: 0.4418\n",
      "Epoch 60/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9060 - loss: 0.2806 - val_accuracy: 0.8981 - val_loss: 0.4333\n",
      "Epoch 61/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9144 - loss: 0.2622 - val_accuracy: 0.8975 - val_loss: 0.4341\n",
      "Epoch 62/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9128 - loss: 0.2643 - val_accuracy: 0.8998 - val_loss: 0.4240\n",
      "Epoch 63/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9122 - loss: 0.2471 - val_accuracy: 0.8935 - val_loss: 0.4564\n",
      "Epoch 64/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9207 - loss: 0.2333 - val_accuracy: 0.8964 - val_loss: 0.4089\n",
      "Epoch 65/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9224 - loss: 0.2355 - val_accuracy: 0.8970 - val_loss: 0.4555\n",
      "Epoch 66/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9176 - loss: 0.2412 - val_accuracy: 0.8947 - val_loss: 0.4439\n",
      "Epoch 67/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9103 - loss: 0.2730 - val_accuracy: 0.8935 - val_loss: 0.4607\n",
      "Epoch 68/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9232 - loss: 0.2345 - val_accuracy: 0.8947 - val_loss: 0.4587\n",
      "Epoch 69/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9189 - loss: 0.2435 - val_accuracy: 0.8981 - val_loss: 0.4512\n",
      "Epoch 70/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9265 - loss: 0.2317 - val_accuracy: 0.8964 - val_loss: 0.5176\n",
      "Epoch 71/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9300 - loss: 0.2099 - val_accuracy: 0.8935 - val_loss: 0.4542\n",
      "Epoch 72/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9267 - loss: 0.2242 - val_accuracy: 0.8975 - val_loss: 0.4534\n",
      "Epoch 73/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9201 - loss: 0.2469 - val_accuracy: 0.8993 - val_loss: 0.4499\n",
      "Epoch 74/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9220 - loss: 0.2262 - val_accuracy: 0.9038 - val_loss: 0.4623\n",
      "Epoch 75/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9283 - loss: 0.2197 - val_accuracy: 0.9015 - val_loss: 0.4626\n",
      "Epoch 76/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9303 - loss: 0.2195 - val_accuracy: 0.9027 - val_loss: 0.4433\n",
      "Epoch 77/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9318 - loss: 0.2133 - val_accuracy: 0.8924 - val_loss: 0.4960\n",
      "Epoch 78/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9288 - loss: 0.2145 - val_accuracy: 0.8975 - val_loss: 0.4935\n",
      "Epoch 79/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9313 - loss: 0.2054 - val_accuracy: 0.8895 - val_loss: 0.4744\n",
      "Epoch 80/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9261 - loss: 0.2252 - val_accuracy: 0.8993 - val_loss: 0.5004\n",
      "Epoch 81/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9352 - loss: 0.1983 - val_accuracy: 0.8964 - val_loss: 0.5178\n",
      "Epoch 82/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9285 - loss: 0.2183 - val_accuracy: 0.9033 - val_loss: 0.4900\n",
      "Epoch 83/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9345 - loss: 0.2072 - val_accuracy: 0.8998 - val_loss: 0.5133\n",
      "Epoch 84/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9341 - loss: 0.1890 - val_accuracy: 0.9027 - val_loss: 0.4983\n",
      "Epoch 85/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9308 - loss: 0.2108 - val_accuracy: 0.9015 - val_loss: 0.4814\n",
      "Epoch 86/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9354 - loss: 0.1903 - val_accuracy: 0.9038 - val_loss: 0.4911\n",
      "Epoch 87/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9278 - loss: 0.2107 - val_accuracy: 0.9078 - val_loss: 0.4680\n",
      "Epoch 88/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9366 - loss: 0.1992 - val_accuracy: 0.9044 - val_loss: 0.4606\n",
      "Epoch 89/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9449 - loss: 0.1735 - val_accuracy: 0.9021 - val_loss: 0.5261\n",
      "Epoch 90/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9373 - loss: 0.1888 - val_accuracy: 0.8970 - val_loss: 0.4892\n",
      "Epoch 91/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9301 - loss: 0.2106 - val_accuracy: 0.9044 - val_loss: 0.4932\n",
      "Epoch 92/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9419 - loss: 0.1930 - val_accuracy: 0.9015 - val_loss: 0.5081\n",
      "Epoch 93/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9376 - loss: 0.1720 - val_accuracy: 0.9021 - val_loss: 0.4928\n",
      "Epoch 94/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9392 - loss: 0.1788 - val_accuracy: 0.9033 - val_loss: 0.4698\n",
      "Epoch 95/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9428 - loss: 0.1702 - val_accuracy: 0.9044 - val_loss: 0.5025\n",
      "Epoch 96/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9338 - loss: 0.1846 - val_accuracy: 0.8993 - val_loss: 0.4944\n",
      "Epoch 97/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9413 - loss: 0.1753 - val_accuracy: 0.9153 - val_loss: 0.4679\n",
      "Epoch 98/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9471 - loss: 0.1583 - val_accuracy: 0.9050 - val_loss: 0.4868\n",
      "Epoch 99/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9463 - loss: 0.1644 - val_accuracy: 0.8930 - val_loss: 0.5354\n",
      "Epoch 100/100\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9423 - loss: 0.1707 - val_accuracy: 0.9073 - val_loss: 0.4707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2d3c0506790>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(52, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten the output of the convolutional layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add dense layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation='softmax')) \n",
    "\n",
    "\n",
    "# Compile the model\n",
    "optimiser = tf.keras.optimizers.Adam(learning_rate = 0.0002)\n",
    "cb = [EarlyStopping(patience=10,monitor='accuracy',mode='max',restore_best_weights=True)]\n",
    "model.compile(optimizer=optimiser, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=16, validation_data=(x_test, y_test), callbacks = cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.17980914e-04, 4.54291221e-05, 1.97404279e-05, ...,\n",
       "        6.63589453e-05, 9.78123248e-01, 1.10564448e-04],\n",
       "       [3.70957736e-08, 3.93185692e-05, 1.84586515e-05, ...,\n",
       "        3.37910365e-06, 6.54780905e-08, 9.99844909e-01],\n",
       "       [8.39249474e-24, 1.43501899e-14, 1.05186305e-17, ...,\n",
       "        3.67826906e-12, 4.88931984e-19, 1.46630873e-14],\n",
       "       ...,\n",
       "       [8.02131797e-17, 1.35121345e-10, 3.62711570e-13, ...,\n",
       "        1.08752477e-15, 3.63796979e-11, 1.77607859e-07],\n",
       "       [7.20335467e-12, 6.69398141e-05, 4.03530187e-08, ...,\n",
       "        1.64667835e-09, 7.15596332e-11, 9.99930143e-01],\n",
       "       [2.95079565e-27, 6.68224024e-28, 1.99836201e-26, ...,\n",
       "        1.13067989e-28, 1.00000000e+00, 2.68387197e-27]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 9, 4, ..., 5, 9, 8], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9072696050372067\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f'Accuracy Score: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       203\n",
      "           1       0.98      0.94      0.96        86\n",
      "           2       0.80      0.86      0.83       183\n",
      "           3       0.86      0.88      0.87       201\n",
      "           4       0.93      0.87      0.90       206\n",
      "           5       0.90      0.97      0.94       193\n",
      "           6       0.91      0.85      0.88        72\n",
      "           7       0.96      0.95      0.95       208\n",
      "           8       0.96      0.94      0.95       165\n",
      "           9       0.89      0.85      0.87       230\n",
      "\n",
      "    accuracy                           0.91      1747\n",
      "   macro avg       0.91      0.91      0.91      1747\n",
      "weighted avg       0.91      0.91      0.91      1747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_report = classification_report(y_test, y_pred_classes)\n",
    "print('Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 25/219\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_train)\n",
    "y_pred_classes = y_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9891195418754474\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_train, y_pred_classes)\n",
    "print(f'Accuracy Score: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       797\n",
      "           1       1.00      1.00      1.00       343\n",
      "           2       0.96      1.00      0.98       817\n",
      "           3       0.98      0.97      0.98       799\n",
      "           4       0.99      0.99      0.99       794\n",
      "           5       1.00      1.00      1.00       807\n",
      "           6       0.99      0.96      0.98       302\n",
      "           7       0.99      0.99      0.99       792\n",
      "           8       0.99      0.99      0.99       764\n",
      "           9       1.00      0.98      0.99       770\n",
      "\n",
      "    accuracy                           0.99      6985\n",
      "   macro avg       0.99      0.99      0.99      6985\n",
      "weighted avg       0.99      0.99      0.99      6985\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_report = classification_report(y_train, y_pred_classes)\n",
    "print('Classification Report:\\n', class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
